[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the Open Machine Learning blog\nThis blog brings you stories about OpenML: why we want to streamline machine learning research, how you can use it, and what we are doing. We are also open to your stories about anything related to open machine learning research, education, and applications. Join the conversation :)."
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html",
    "title": "Experiments with Temperature",
    "section": "",
    "text": "Over the past few months at OpenML, we have been experimenting with LLM models in an attempt to improve the search experience for our users. While our existing implementation uses ElasticSearch, we wanted to also have the option of having a more “semantic” search experience.\nAside from the usual RAG pipeline that everyone and their grandparents seems to be using these days, we also wanted to experiment with using an LLM to semi-automatically generate filters for our search queries. While it may not seem like a big feature, it is something that has always been a bit of an annoyance for some of our users.\nSo what does this entail? Consider the interface we have at the moment. We have a search bar at the top, and subsequently a bunch of filters that users can use to narrow down their search. While this works pretty well as is, how about trying to automate it a bit.\nIn summary, we want a query like “find me a large dataset with multiple classes of flowers” to automatically generate filters like “classification”, “multiclass”, “sort by size of dataset” etc."
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#temperature",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#temperature",
    "title": "Experiments with Temperature",
    "section": "Temperature",
    "text": "Temperature\nThink about the first time you used ChatGPT. What stood out to you? Was it how well it could elaborate on a topic? Or was it how creative it could be? The temperature parameter in LLMs is what controls this.\nHow can we control creativity? Well, saying that we can directly control creativity is a bit of a stretch. We can however use a workaround.\nDo you remember the softmax function? The function that takes a vector of arbitrary real-valued scores and squashes it into a vector of probabilities that sum to 1. The inputs to the softmax function are the unnormalized log likelikhoods or the raw per class score assigned by the model.\nThe softmax function is defined as: \\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{k} e^{x_j}} \\]\nIf we want more control over the distribution of the probabilities, we can use a temperature parameter. This would look like: \\[ \\text{softmax}(x_i) = \\frac{e^{x_i/T}}{\\sum_{j=1}^{k} e^{x_j/T}} \\] where \\(T\\) is the temperature parameter.\n\nIf \\(T = 1\\), the softmax function is the same as the original softmax function.\nIf \\(T &gt; 1\\), the probabilities will become “flatter”. Since the difference between the probabilities will be less, the model can be more exploratory aka more creative.\nIf \\(T &lt; 1\\), the distribution of the probabilities are “peakier”. There will be a higher difference between the probabilities, leading to the model being more confident in its predictions, but also less creative.\n\n\nVisualizing Temperature using Softmax\n\nfrom tqdm import tqdm\nimport regex as re\n# LangChain supports many other chat models. Here, we're using Ollama\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom typing import List, Dict, Any\nimport numpy as np  \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd \nsns.set_theme(style=\"white\")\n\n\ndef softmax(input, t=1.0):\n  ex = np.exp(input/t)\n  sum = np.sum(ex, axis=0)\n  return ex / sum\n\n\n# plot softmax over a range of inputs\nx = np.arange(0,1.0, 0.01)\nt = np.array([0.1,.5, .8, 1.0])\ny = np.array([softmax(x, ti) for ti in t])\n\n# Create a DataFrame for Seaborn\ndata = pd.DataFrame({\n    'x': np.tile(x, len(t)),\n    'softmax': np.concatenate(y),\n    't': np.repeat(t, len(x))\n})\n\n# Plotting with Seaborn\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=data, x='x', y='softmax', hue='t', palette='viridis')\nplt.xlabel('x')\nplt.ylabel('softmax(x)')\nplt.title('Softmax Function for Different Values of t')\nplt.legend(title='t')\nplt.show()"
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#creating-the-experimental-setup",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#creating-the-experimental-setup",
    "title": "Experiments with Temperature",
    "section": "Creating the Experimental Setup",
    "text": "Creating the Experimental Setup\nNow, we can focus on testing the effects of temperature for our use case. We are using the llama3 model for our experiments. The experiments are being run on a 2023 MacBook Pro with an M3 chip and 18GB memory.\n\nDefining a Prompt\nWe need to first think of a prompt that we can use for our experiments. This prompt can be thought of as an instruction that the model uses along with the query to generate answers. To make it easier for us to use, we only want one/two word answers and for now we are only focusing on a small subset of the filters that we want our model to understand.\n\nprompt = \"\"\"User Query : {query}\nBased on the query, answer the following questions one by one in one or two words only and a maximum of two with commas only if asked for. Use only the information given and do not make up answers - \nDoes the user care about the size of the dataset? Yes/No and if yes, ascending/descending.\nDoes the user want to sort by number of downloads? Yes/No.\nDoes the user care about missing values? Yes/No.\nIf it seems like the user wants a classification dataset, is it binary/multi-class/multi-label? If not, say none.\n\"\"\"\n\n\nquery = \"Find me a big classification dataset about mushrooms\"\n\n\n\nCreating a Chain\nSince we are using the langchain and ollama libraries for our experiments, we follow their API and create a chain. The template uses string formatting to insert the prompt and the query into the chain.\n\ndef create_chain(prompt , temperature, llm_model = \"llama3\"):\n    prompt = ChatPromptTemplate.from_template(prompt)\n    llm = ChatOllama(model=llm_model, temperature=temperature)\n    chain = prompt | llm | StrOutputParser()\n    return chain\n\n\n\nParsing the Results\nTo make it easier for us to analyze the results, we generate an example answer and then see see if any further processing is needed.\n\n# functiont to parse responses like this to a list of yes/no/none/yes,aescending/no etc\ndef parse_response(response):\n    # split by new line and remove first two lines (here are the answers:)\n    response = response.split('\\n')[2::]\n    # if response has a question mark, split by question mark and remove empty strings\n    for i in range(len(response)):\n        if '?' in response[i]:\n            response[i] = response[i].split('?')[1].strip()\n    # replace full stops with empty strings\n    response = [x.replace('.','') for x in response]\n    response = [x for x in response if x]\n    return response\n\n\nchain = create_chain(prompt, 0.5)\nresponse = chain.invoke({\"query\": query})\nprint(response)\n\nHere are the answers:\n\n1. Does the user care about the size of the dataset?\nYes, ascending.\n\n2. Does the user want to sort by number of downloads?\nNo\n\n3. Does the user care about missing values?\nNo\n\n4. Is it a classification dataset? If so, is it binary/multi-class/multi-label?\nYes, multi-class\n\n\nYay, it works. We now write a function to generate results for different temperatures.\n\ndef generate_results_for_temp(query:str, range_of_temps : np.ndarray) -&gt; List[List[str]]:\n    results = []\n    for temperature in tqdm(range_of_temps):\n        chain = create_chain(prompt, temperature)\n        response = chain.invoke({\"query\": query})\n        results.append(parse_response(response))\n    return results"
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#running-the-experiments-and-plotting-results",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#running-the-experiments-and-plotting-results",
    "title": "Experiments with Temperature",
    "section": "Running the Experiments and Plotting Results",
    "text": "Running the Experiments and Plotting Results\nIt is time to run the experiments and plot the results. We write a function to plot the results in a stripplot to see the distribution of the answers for different temperatures.\n\ndef plot_yes_no(df: pd.DataFrame, title:str) -&gt; None:\n    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n    fig.suptitle(title)\n    sns.stripplot(data=df, x='size', y='temperature', ax=axs[0, 0], hue='size')\n    sns.stripplot(data=df, x='sort_by_downloads', y='temperature', ax=axs[0, 1], hue='sort_by_downloads')\n    sns.stripplot(data=df, x='missing_values', y='temperature', ax=axs[1, 0], hue='missing_values')\n    sns.stripplot(data=df, x='classification_type', y='temperature', ax=axs[1, 1], hue='classification_type')\n    # tilt x axis labels\n    for ax in axs.flat:\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n    plt.show()\n\nSometimes, the model returns an extra field, we combine the last two fields to plot the results. (This is a bit of a hack, but it works for now and is ONLY used for plotting)\n\ndef combine_last_two_elements(lst):\n    # Check if the list has at least two elements\n    if len(lst) &gt; 4:\n        # Combine the last two elements with a space separator\n        combined_element = lst[-2] + ' ' + lst[-1]\n\n        # Create a new list with combined element instead of the last two\n        return lst[:-2] + [combined_element]\n    else:\n        return lst\n\n\nExperiment 1\nOut first experiment is a rather simple query, “Find me a big classification dataset about mushrooms”. As you can probably guess, we are looking for a dataset that is large, is a classification dataset and is about mushrooms.\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a big classification dataset about mushrooms\"\nresults1 = generate_results_for_temp(query, range_of_temps)\n\n100%|██████████| 20/20 [00:49&lt;00:00,  2.49s/it]\n\n\n\nresults1 = [[y for y in x if all(sub not in y for sub in [\"If\", \":\"])] for x in results1]\n\n\ndf = pd.DataFrame(results1, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, title = query)\n\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n\n\n\n\n\n\n\n\n\nRather interesting don’t you think? At higher temperatures, the model gets the answers wrong. Even at a temperature slightly above 0.1, the model starts adding extra information to it’s answers.\nDid you notice that I tried to remove sentences that started with “If”? There are more examples of this later, but this is because at higher temperatures, the model tends to add random sentences to the answers and this makes it quite hard to plot them.\n\n\nExperiment 2\nOur second experiment is super easy. “Find me a dataset that has a lot of missing values and order by number of downloads”. As you can obviously guess, we are looking for a dataset that has a lot of missing values and we want to order the results by the number of downloads.\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that has a lot of missing values and order by number of downloads\"\nresults2 = generate_results_for_temp(query, range_of_temps)\nresults2 = [[y for y in x if \"so\" not in y] for x in results2]\n\n100%|██████████| 20/20 [00:34&lt;00:00,  1.74s/it]\n\n\n\ndf = pd.DataFrame(results2, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, title = query)\n\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n\n\n\n\n\n\n\n\n\nHmm, same as before. The model starts adding extra information at higher temperatures and starts getting the answers wrong. (Yes, No?? ) What kind of answer is that?"
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#experiment-3",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#experiment-3",
    "title": "Experiments with Temperature",
    "section": "Experiment 3",
    "text": "Experiment 3\n\nNow a slightly more complex query. “Find me a dataset that has 10 classes and sort by number of downloads”. We want it to understand that we want a multiclass classification dataset and we want to sort the results by the number of downloads.\n\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that has 10 classes and sort by number of downloads\"\nresults3 = generate_results_for_temp(query, range_of_temps)\n\n100%|██████████| 20/20 [00:55&lt;00:00,  2.80s/it]\n\n\n\nresults3 = [combine_last_two_elements(x) for x in results3]\n\n\ndf = pd.DataFrame(results3, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, title = query)\n\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n\n\n\n\n\n\n\n\n\nThis seems to have been very easy for the model. But as always, the model starts adding extra information at higher temperatures. A lot of extra information in fact. Even though the prompt says to ONLY answer with one or two words"
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#experiment-4",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#experiment-4",
    "title": "Experiments with Temperature",
    "section": "Experiment 4",
    "text": "Experiment 4\n\n“Find me a dataset that 2 classes and is a big dataset”. You know the drill by now. We want a binary classification dataset that is large.\n\n\nrange_of_temps = np.linspace(0, 1, 20)\nquery = \"Find me a dataset that 2 classes and is a big dataset\"\nresults4 = generate_results_for_temp(query, range_of_temps)\n\n100%|██████████| 20/20 [00:42&lt;00:00,  2.14s/it]\n\n\n\nresults4 = [combine_last_two_elements(x) for x in results4]\n\n\ndf = pd.DataFrame(results4, columns = ['size', 'sort_by_downloads', 'missing_values', 'classification_type'])\ndf['temperature'] = range_of_temps\nplot_yes_no(df, title = query)\n\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n/var/folders/_f/ng_zp8zj2dgf828sb6s5wdb00000gn/T/ipykernel_42184/3521201435.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n\n\n\n\n\n\n\n\n\nNotice how some things changed? At higher temperatures, we get extended answers."
  },
  {
    "objectID": "posts/Experiments-with-temperature/experiments_with_temp.html#conclusion",
    "href": "posts/Experiments-with-temperature/experiments_with_temp.html#conclusion",
    "title": "Experiments with Temperature",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we can see that we should probably stick to lower temperatures for our use case. As we go higher, the model starts being more “creative” and either adds extra information to the answers or gets them wrong. While this behaviour might be useful in cases like creative writing, it is not something we want in our search.\nUsing LLMs can sometimes be a bit of a hit or miss. But of course, learning to control it’s parameters can help us get the most out of it. This blog post was just a simple experiment, but in the deluge of content made by people who have no idea what Softmax is, I hope this was helpful."
  },
  {
    "objectID": "posts/2019-10-24-OpenML-workshop-at-Dagstuhl/2019-10-24-OpenML-workshop-at-Dagstuhl.html",
    "href": "posts/2019-10-24-OpenML-workshop-at-Dagstuhl/2019-10-24-OpenML-workshop-at-Dagstuhl.html",
    "title": "OpenML workshop at Dagstuhl",
    "section": "",
    "text": "October 2019\nTwice a year the OpenML community meets for a workshop/hackathon/unconference. We improve the platform, discuss and learn for 5 days. If that sounds interesting to you, get in touch or follow updates on meet.openml.org.\nThis time the workshop took place at Dagstuhl, a great place for Seminars.\n\n\n\n\nGroup pictures\n\n\nWe had several breakouts where workshop attendees can join to learn, discuss and progress OpenML. In the following we discuss some of the topics we touched.\nScience projects\nBrainstorm on scientific projects to do with OpenML. Prioritize impactful, well-defined research ideas. We came up with quite a long list of very promising research questions that should actually be quite easy to answer based on OpenML. Many of these are along the lines of empirically providing evidence to verify or bust commonly-held beliefs in the community. Frank would gladly hire a strong postdoc (or quite independent PhD student) and maybe a research engineer, to work on these scientific questions under the umbrella of “evidence-based machine learning with OpenML”.\nBenchmarking using OpenML\nDefine guidelines on how to define world-class benchmarks and how to run them.\nDiverse datasets\nNew and more diverse datasets.\nDataset quality\nHow to measure data quality and how to improve the quality of datasets on OpenML.\nOpenML use cases for novices\nShortlist common use cases and start writing accessible blog posts for novice users. If you are a OpenML newbie we need your help with this topic.\nThe output of this breakout will be at least one blog post. Keep an eye out for them here :)\nRunning a competition using OpenML\nWe assessed the current issues with running in-class competitions for teaching purposes using OpenML (biggest one: easy use for non-developers) and brainstormed on a new competition format with related competitions, one for each component of a solution, such as HPO, creating good meta-features, creating a good search space, etc.\nPlanning future workshops\nDecide on location and timing for the next couple of workshops. The next OpenML workshop will be in Spring (week of March 30th or week of April 14th) close to Munich. For updates check http://meet.openml.org. The workshop will be cohosted with some other Open Source Machine Learning projects.\nFurthermore a workshop in Austin is being planned for next summer and a datathon is in planning. We are planning to organize dev sprints at various PyCons next year.\nNew frontend\nFeedback session on new frontend, Additional visualization for datasets\nFuture of client APIs\nCurrently, a lot of resources are bound developing different client APIs, such as the Python API, R API and Java API. We discussed how we can better share work and code between the different APIs and the server. For now we are working on automatically generating the Swagger API documentation from the PHP function documentation, which in turn will allow us to generate (documented) parts of the APIs, reduce the need for maintenance and will help to spread updates on the API faster.\nFlow 2.0 design\nCurrent flow design used in OpenML was inspired by Weka, but through time many limitations have been identified, primarily that existing flow does not allow duplicate use of same component and that it cannot express DAG-based ML programs. We started working on a new specification building on insights from mlr3 and d3m projects, centered around DAG representation. Current plan is to prepare a draft specification and implement prototype converters between other systems and this new specification. Once we do that we will re-evaluate the amount of work it took to build those prototypes and how well the specification satisfied those other systems.\nRandom Bot\nThe LRZ in Munich provided us with CPU time during the SuperMUC-NG supercomputer test phase, which we used to perform experiments of popular machine learning algorithms with random hyperparameter configurations. This resulted in millions of data points on more than a hundred datasets that we will analyse and publish. The data can be used to learn about typical behaviour of different learners across different datasets, and to construct surrogate models for tuning algorithm benchmarks.\nAutoML Benchmark (Janek)\nA study was created containing 76 binary and multiclass tasks of reasonable difficulty.\nThese can be used as a more difficult version of OpenML-100 or in amlb a platform for reproducible benchmarking of AutoML systems.\nR API\nShort session on how the R api will (need to) change. The main issue discussed was that the OpenML R package runs with mlr and breaks when the new package (mlr3) is loaded. We will update the current OpenML R package to work with mlr3. At the same time we are thinking about a vision for a rewrite of the OpenML R package.\nPython API\nWe made a lot of improvements to the Python API over the week, with over 20 PRs merged! We’ve added more examples on how to use the package, fixed bugs, improved documentation and refactored code. In the coming days we’re going to make all these improvements available in a new PyPI release. For those looking for a higher level overview of the package, we will publish a paper next week which highlights use-cases, its software design, and project structure.\nBenchmarking paper\nWe are working on a comprehensive paper using sklearn, mlr and WEKA, which should demonstrate how OpenML can be used for proper benchmarking and analysis.\nGuidelines / Overfitting / Comparable Metalearning\nThere are plans for writing a guidelines and pitfalls paper on benchmarking, meta-overfitting and statistical analysis of results on OpenML.\nData Formats\nCurrently OpenML supports only tabular data in ARFF data format. This is very limiting for many ML tasks. We discussed and explored other data formats we could use as the future next data format. We will post a separate blog post about our process and insights.\nFunding\nOpenML is looking for funding (developers). New ideas on obtaining funding are very welcome. We discussed some ideas: American funding (we need a collaboration partner); ALICE / CLAIRE; Companies. We are a foundation now, which might make it easier.\nSome of the PIs (in particular Bernd Bischl, Frank Hutter and Dawn Song) in the project offer positions with a mix of ML research and development. Contact them if you are interested!\n\n\n\n\nSketch of the OpenML infrastructure (left), Breakout overview of Tuesday (right)\n\n\nWe had some talks at the workshop as well:\n\nMitar Milutinovic: A short introduction to Data Driven Discovery (D3M)\nYiwen Zhu and Markus Weimer: Large-scale analysis of Jupyter notebooks\nMartin Binder, Michel Lang, Florian Pfisterer, Bernd Bischl: Pipelining with mlr3\n\n… and lots of fun…\n\n\n\nOn the hike\n\n\nWanna join the OpenML community? Get in touch!"
  },
  {
    "objectID": "posts/2019-10-26-OpenML-Machine-Learning-as-a-community/2019-10-26-OpenML-Machine-Learning-as-a-community.html",
    "href": "posts/2019-10-26-OpenML-Machine-Learning-as-a-community/2019-10-26-OpenML-Machine-Learning-as-a-community.html",
    "title": "OpenML - Machine Learning as a community",
    "section": "",
    "text": "OpenML is an online Machine Learning (ML) experiments database accessible to everyone for free. The core idea is to have a single repository of datasets and results of ML experiments on them. Despite having gained a lot of popularity in recent years, with a plethora of tools now available, the numerous ML experimentations continue to happen in silos and not necessarily as one whole shared community. In this post, we shall try to get a brief glimpse of what OpenML offers and how it can fit our current Machine Learning practices.\nLet us jump straight at getting our hands dirty by building a simple machine learning model. If it is simplicity we are looking for, it has to be the Iris dataset that we shall work with. In the example script below, we are going to load the Iris dataset available with scikit-learn, use 10-fold cross-validation to evaluate a Random Forest of 10 trees. Sounds trivial enough and is indeed less than 10 lines of code.\n\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n# Loading Iris dataset\nX, y = datasets.load_iris(return_X_y=True)\nprint(X.shape, y.shape)\n\n(150, 4) (150,)\n\n\n\n# Initializing a Random Forest with \n# arbitrary hyperparameters\n# max_depth kept as 2 since Iris has\n# only 4 features\nclf = RandomForestClassifier(n_estimators=10, max_depth=2)\n\n\nscores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\nprint(\"Mean score : {:.5f}\".format(scores.mean()))\n\nMean score : 0.94000\n\n\nA simple script and we achieve a mean accuracy of 95.33%. That was easy. It is really amazing how far we have come with ML tools that make it easy to get started. As a result, we have hundreds of thousands of people working with these tools every day. That inevitably leads to the reinvention of the wheel. The tasks that each individual ML practitioner performs often have significant overlaps and can be omitted by reusing what someone from the community has done already. At the end of the day, we didn’t build a Random Forest model all the way from scratch. We gladly reused code written by generous folks from the community. The special attribute of our species is the ability to work as a collective wherein our combined intellect becomes larger than the individual sum of parts. Why not do the same for ML? I mean, can I see what other ML practitioners have done to get better scores on the Iris dataset?\nAnswering this is one of the targets of this post. We shall subsequently explore if this can be done, with the help of OpenML. However, first, we shall briefly familiarize ourselves with few terminologies and see how we can split the earlier example we saw into modular components.\n\nOpenML Components\n\nImage source: https://medium.com/open-machine-learning/openml-1e0d43f0ae13\nDataset: OpenML houses over 2k+ active datasets for various regression, classification, clustering, survival analysis, stream processing tasks and more. Any user can upload a dataset. Once uploaded, the server computes certain meta-features on the dataset - Number of classes, Number of missing values, Number of features, etc. With respect to our earlier example, the following line is the equivalent of fetching a dataset from OpenML.\n\nX, y = datasets.load_iris(return_X_y=True)\n\nTask: A task is linked to a specific dataset, defining what the target/dependent variable is. Also specifies evaluation measures such as - accuracy, precision, area under curve, etc. or the kind of estimation procedure to be used such as - 10-fold cross-validation, n% holdout set, etc. With respect to our earlier example, the parameters to the following function call capture the idea of a task.\n\nscores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n\nFlow: Describes the kind of modelling to be performed. It could be a flow or a series of steps, i.e., a scikit-learn pipeline. For now, we have used a simple Random Forest model which is the flow component here.\n\nclf = RandomForestClassifier(n_estimators=10, max_depth=2)\n\nRun: Pairs a flow and task together which results in a run. The run has the predictions which are turned into evaluations by the server. This is effectively captured by the execution of the line:\n\nscores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n\nNow, this may appear a little obfuscating given that we are trying to compartmentalize a simple 10-line code which works just fine. However, if we take a few seconds to go through the 4 components explained above, we can see that it makes our training of a Random Forest on Iris a series of modular tasks. Modules are such a fundamental concept in Computer Science. They are like Lego blocks. Once we have modules, it means we can plug and play at ease. The code snippet below attempts to rewrite the earlier example using the ideas of the OpenML components described, to give a glimpse of what we can potentially gain during experimentations.\n\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\nDATASET component\n\n# To load IRIS dataset as a dataset module/component\ndef dataset():\n    X, y = datasets.load_iris(return_X_y=True)\n    return X, y\n\n\n\nTASK component\n\n# Tasks here define the number of cross-validation folds\n# and the scoring metric to be used for evaluation\ndef task_1(f):\n    X, y = dataset()  # loads IRIS\n    return cross_val_score(f, X, y, cv=5, \n                           scoring='accuracy')\n\ndef task_2(f):\n    X, y = dataset()  # loads IRIS\n    return cross_val_score(f, X, y, cv=15, \n                           scoring='balanced_accuracy')\n\n\n\nFLOW component\n\n# Flows determine the modelling technique to be applied\n# Helps define a model irrespective of dataset or tasks\ndef flow_1():\n    clf = RandomForestClassifier(n_estimators=10, max_depth=2)\n    return clf\n\ndef flow_2():\n    clf = SVC(gamma='auto', kernel='linear')\n    return clf \n\n\n\nRUN component\n\n# Runs essentially evaluates a task-flow pairing \n# and therefore in effect executs the modelling \n# of a dataset as per the task task definition\ndef run(task, flow):\n    return task(flow)\n\n\n# Results for Random Forest\nrf_task_1 = run(task_1, flow_1())\nrf_task_2 = run(task_2, flow_1())\nprint(\"RF using task 1: {:&lt;.5}; task 2: {:&lt;.5}\".format(rf_task_1.mean(), rf_task_2.mean()))\n\n# Results for SVM\nsvm_task_1 = run(task_1, flow_2())\nsvm_task_2 = run(task_2, flow_2())\nprint(\"SVM using task 1: {:&lt;.5}; task 2: {:&lt;.5}\".format(svm_task_1.mean(), svm_task_2.mean()))\n\nRF using task 1: 0.95333; task 2: 0.94444\nSVM using task 1: 0.98; task 2: 0.97222\n\n\nWe can, therefore, compose various different tasks, flows, which are independent operations. Runs can then pair any such task and flow to construct an ML workflow and return the evaluated scores. This approach can help us define such components one-time, and we can extend this for any combination of a dataset, model, and for any number of evaluations in the future. Imagine if the entire ML community defines such tasks and various simple to complicated flows that they use in their daily practice. We can build custom working ML pipeline and even get to compare performances of our techniques on the same task with others! OpenML aims exactly for that. In the next section of this post, we shall scratch the surface of OpenML to see if we can actually do with OpenML what it promises.\n\n\n\nUsing OpenML\nOpenML-Python can be installed using pip or by cloning the git repo and installing the current development version. So shall we then install OpenML? ;) It will be beneficial if the code snippets are tried out as this post is read. A consolidated Jupyter notebook with all the code can be found here.\nNow that we have OpenML, let us jump straight into figuring out how we can get the Iris dataset from there. We can always browse theOpenML website and search for Iris. That is the easy route. Let us get familiar with the programmatic approach and learn how to fish instead. The OpenML-Python API can be found here.\n\nRetrieving Iris from OpenML\nIn the example below, we will list out all possible datasets available in OpenML. We can choose the output format. I’ll go with dataframe so that we obtain a pandas DataFrame and can get a neat tabular representation to search and sort specific entries.\n\nimport openml\n\nimport numpy as np\nimport pandas as pd\n\n\n# Fetching the list of all available datasets on OpenML\nd = openml.datasets.list_datasets(output_format='dataframe')\nprint(d.shape)\n\n# Listing column names or attributes that OpenML offers\nfor name in d.columns:\n    print(name)\n\n(3073, 16)\ndid\nname\nversion\nuploader\nstatus\nformat\nMajorityClassSize\nMaxNominalAttDistinctValues\nMinorityClassSize\nNumberOfClasses\nNumberOfFeatures\nNumberOfInstances\nNumberOfInstancesWithMissingValues\nNumberOfMissingValues\nNumberOfNumericFeatures\nNumberOfSymbolicFeatures\n\n\n\nprint(d.head())\n\n   did        name  version uploader  status format  MajorityClassSize  \\\n2    2      anneal        1        1  active   ARFF              684.0   \n3    3    kr-vs-kp        1        1  active   ARFF             1669.0   \n4    4       labor        1        1  active   ARFF               37.0   \n5    5  arrhythmia        1        1  active   ARFF              245.0   \n6    6      letter        1        1  active   ARFF              813.0   \n\n   MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  \\\n2                          7.0                8.0              5.0   \n3                          3.0             1527.0              2.0   \n4                          3.0               20.0              2.0   \n5                         13.0                2.0             13.0   \n6                         26.0              734.0             26.0   \n\n   NumberOfFeatures  NumberOfInstances  NumberOfInstancesWithMissingValues  \\\n2              39.0              898.0                               898.0   \n3              37.0             3196.0                                 0.0   \n4              17.0               57.0                                56.0   \n5             280.0              452.0                               384.0   \n6              17.0            20000.0                                 0.0   \n\n   NumberOfMissingValues  NumberOfNumericFeatures  NumberOfSymbolicFeatures  \n2                22175.0                      6.0                      33.0  \n3                    0.0                      0.0                      37.0  \n4                  326.0                      8.0                       9.0  \n5                  408.0                    206.0                      74.0  \n6                    0.0                     16.0                       1.0  \n\n\nThe column names indicate that they contain the meta-information about each of the datasets, and at this instance, we have access to 2958 datasets as indicated by the shape of the dataframe. We shall try searching for ‘iris’ in the column name and also use the version column to sort the results.\n\n# Filtering dataset list to have 'iris' in the 'name' column\n# then sorting the list based on the 'version'\nd[d['name'].str.contains('iris')].sort_values(by='version').head()\n\n\n\n\n\n\n\n\ndid\nname\nversion\nuploader\nstatus\nformat\nMajorityClassSize\nMaxNominalAttDistinctValues\nMinorityClassSize\nNumberOfClasses\nNumberOfFeatures\nNumberOfInstances\nNumberOfInstancesWithMissingValues\nNumberOfMissingValues\nNumberOfNumericFeatures\nNumberOfSymbolicFeatures\n\n\n\n\n61\n61\niris\n1\n1\nactive\nARFF\n50.0\n3.0\n50.0\n3.0\n5.0\n150.0\n0.0\n0.0\n4.0\n1.0\n\n\n41950\n41950\niris_test_upload\n1\n4030\nactive\nARFF\n50.0\n3.0\n50.0\n3.0\n5.0\n150.0\n0.0\n0.0\n4.0\n1.0\n\n\n42261\n42261\niris-example\n1\n348\nactive\nARFF\n50.0\nNaN\n50.0\n3.0\n5.0\n150.0\n0.0\n0.0\n4.0\n1.0\n\n\n451\n451\nirish\n1\n2\nactive\nARFF\n278.0\n10.0\n222.0\n2.0\n6.0\n500.0\n32.0\n32.0\n2.0\n4.0\n\n\n969\n969\niris\n3\n2\nactive\nARFF\n100.0\n2.0\n50.0\n2.0\n5.0\n150.0\n0.0\n0.0\n4.0\n1.0\n\n\n\n\n\n\n\nOkay, so the iris dataset with the version as 1 has an ID of 61. For verification, we can check the website for dataset ID 61. We can see that it is the original Iris dataset which is of interest to us - 3 classes of 50 instances, with 4 numeric features. However, we shall retrieve the same information, as promised, programmatically.\n\niris = openml.datasets.get_dataset(61)\niris\n\nOpenML Dataset\n==============\nName..........: iris\nVersion.......: 1\nFormat........: ARFF\nUpload Date...: 2014-04-06 23:23:39\nLicence.......: Public\nDownload URL..: https://www.openml.org/data/v1/download/61/iris.arff\nOpenML URL....: https://www.openml.org/d/61\n# of features.: 5\n# of instances: 150\n\n\n\niris.features\n\n{0: [0 - sepallength (numeric)],\n 1: [1 - sepalwidth (numeric)],\n 2: [2 - petallength (numeric)],\n 3: [3 - petalwidth (numeric)],\n 4: [4 - class (nominal)]}\n\n\n\nprint(iris.description)\n\n**Author**: R.A. Fisher  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Iris) - 1936 - Donated by Michael Marshall  \n**Please cite**:   \n\n**Iris Plants Database**  \nThis is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.  (See Duda & Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is     linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\nPredicted attribute: class of iris plant.  \nThis is an exceedingly simple domain.  \n \n### Attribute Information:\n    1. sepal length in cm\n    2. sepal width in cm\n    3. petal length in cm\n    4. petal width in cm\n    5. class: \n       -- Iris Setosa\n       -- Iris Versicolour\n       -- Iris Virginica\n\n\nWith the appropriate dataset available, let us briefly go back to the terminologies we discussed earlier. We have only used the dataset component so far. The dataset component is closely tied with the task component. To reiterate, the task would describe how the dataset will be used.\n\n\nRetrieving relevant tasks from OpenML\nWe shall firstly list all available tasks that work with the Iris dataset. However, we are only treating Iris as a supervised classification problem and hence will filter accordingly. Following which, we will collect only the task IDs of the tasks relevant to us.\n\ndf = openml.tasks.list_tasks(data_id=61, output_format='dataframe')\ndf.head()\n\n\n\n\n\n\n\n\ntid\nttid\ndid\nname\ntask_type\nstatus\nestimation_procedure\nevaluation_measures\nsource_data\ntarget_feature\n...\nNumberOfFeatures\nNumberOfInstances\nNumberOfInstancesWithMissingValues\nNumberOfMissingValues\nNumberOfNumericFeatures\nNumberOfSymbolicFeatures\nnumber_samples\ncost_matrix\nquality_measure\ntarget_value\n\n\n\n\n59\n59\n1\n61\niris\nSupervised Classification\nactive\n10-fold Crossvalidation\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n118\n118\n3\n61\niris\nLearning Curve\nactive\n10 times 10-fold Learning Curve\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\n4\nNaN\nNaN\nNaN\n\n\n289\n289\n1\n61\niris\nSupervised Classification\nactive\n33% Holdout set\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n1758\n1758\n3\n61\niris\nLearning Curve\nactive\n10-fold Learning Curve\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\n4\nNaN\nNaN\nNaN\n\n\n1823\n1823\n1\n61\niris\nSupervised Classification\nactive\n5 times 2-fold Crossvalidation\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n# Filtering only the Supervised Classification tasks on Iris\ndf.query(\"task_type=='Supervised Classification'\").head()\n\n\n\n\n\n\n\n\ntid\nttid\ndid\nname\ntask_type\nstatus\nestimation_procedure\nevaluation_measures\nsource_data\ntarget_feature\n...\nNumberOfFeatures\nNumberOfInstances\nNumberOfInstancesWithMissingValues\nNumberOfMissingValues\nNumberOfNumericFeatures\nNumberOfSymbolicFeatures\nnumber_samples\ncost_matrix\nquality_measure\ntarget_value\n\n\n\n\n59\n59\n1\n61\niris\nSupervised Classification\nactive\n10-fold Crossvalidation\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n289\n289\n1\n61\niris\nSupervised Classification\nactive\n33% Holdout set\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n1823\n1823\n1\n61\niris\nSupervised Classification\nactive\n5 times 2-fold Crossvalidation\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n1939\n1939\n1\n61\niris\nSupervised Classification\nactive\n10 times 10-fold Crossvalidation\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n1992\n1992\n1\n61\niris\nSupervised Classification\nactive\nLeave one out\npredictive_accuracy\n61\nclass\n...\n5\n150\n0\n0\n4\n1\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n# Collecting all relevant task_ids\ntasks = df.query(\"task_type=='Supervised Classification'\")['tid'].to_numpy()\nprint(len(tasks))\n\n13\n\n\nThat settles the task component too. Notice how for one dataset (61), we obtain 11 task IDs which are of interest to us. This should illustrate the one-to-many relationship that dataset-task components can have. We have 2 more components to explore - flows, runs. We could list out all possible flows and filter out the ones we want, i.e., Random Forest. However, let us instead fetch all the evaluations made on the Iris dataset using the 11 tasks we collected above.\nWe shall subsequently work with the scikit-learn based task which has been uploaded/used the most. We shall then further filter out the list of evaluations from the selected task (task_id=59 in this case), depending on if Random Forest was used.\n\n# Listing all evaluations made on the 11 tasks collected above\n# with evaluation metric as 'predictive_accuracy'\ntask_df = openml.evaluations.list_evaluations(function='predictive_accuracy', task=tasks, output_format='dataframe')\ntask_df.head()\n\n\n\n\n\n\n\n\nrun_id\ntask_id\nsetup_id\nflow_id\nflow_name\ndata_id\ndata_name\nfunction\nupload_time\nuploader\nuploader_name\nvalue\nvalues\narray_data\n\n\n\n\n0\n81\n59\n12\n67\nweka.BayesNet_K2(1)\n61\niris\npredictive_accuracy\n2014-04-07 00:05:11\n1\njanvanrijn@gmail.com\n0.940000\nNone\nNone\n\n\n1\n161\n59\n13\n70\nweka.SMO_PolyKernel(1)\n61\niris\npredictive_accuracy\n2014-04-07 00:55:32\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n2\n234\n59\n1\n56\nweka.ZeroR(1)\n61\niris\npredictive_accuracy\n2014-04-07 01:33:24\n1\njanvanrijn@gmail.com\n0.333333\nNone\nNone\n\n\n3\n447\n59\n6\n61\nweka.REPTree(1)\n61\niris\npredictive_accuracy\n2014-04-07 06:26:27\n1\njanvanrijn@gmail.com\n0.926667\nNone\nNone\n\n\n4\n473\n59\n18\n77\nweka.LogitBoost_DecisionStump(1)\n61\niris\npredictive_accuracy\n2014-04-07 06:39:27\n1\njanvanrijn@gmail.com\n0.946667\nNone\nNone\n\n\n\n\n\n\n\n\n# Filtering based on sklearn (scikit-learn)\ntask_df = task_df[task_df['flow_name'].str.contains(\"sklearn\")]\ntask_df.head()\n\n\n\n\n\n\n\n\nrun_id\ntask_id\nsetup_id\nflow_id\nflow_name\ndata_id\ndata_name\nfunction\nupload_time\nuploader\nuploader_name\nvalue\nvalues\narray_data\n\n\n\n\n144\n1849043\n59\n29015\n5500\nsklearn.ensemble.forest.RandomForestClassifier...\n61\niris\npredictive_accuracy\n2017-03-03 17:10:12\n1\njanvanrijn@gmail.com\n0.946667\nNone\nNone\n\n\n145\n1853409\n59\n30950\n5873\nsklearn.pipeline.Pipeline(Imputer=openml.utils...\n61\niris\npredictive_accuracy\n2017-03-21 22:08:01\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n146\n6130126\n59\n4163633\n7108\nsklearn.model_selection._search.RandomizedSear...\n61\niris\npredictive_accuracy\n2017-08-21 11:07:40\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n147\n6130128\n59\n4163634\n7108\nsklearn.model_selection._search.RandomizedSear...\n61\niris\npredictive_accuracy\n2017-08-21 11:08:06\n1\njanvanrijn@gmail.com\n0.946667\nNone\nNone\n\n\n148\n6715383\n59\n4747289\n7117\nsklearn.model_selection._search.RandomizedSear...\n61\niris\npredictive_accuracy\n2017-09-01 02:56:44\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n\n\n\n\n\n\n# Counting frequency of the different tasks used to\n# solve Iris as a supervised classification using scikit-learn\ntask_df['task_id'].value_counts()\n\n59       1985\n10107      25\n289         1\nName: task_id, dtype: int64\n\n\n\n# Retrieving the most used task\nt = openml.tasks.get_task(59)\nt\n\nOpenML Classification Task\n==========================\nTask Type Description: https://www.openml.org/tt/1\nTask ID..............: 59\nTask URL.............: https://www.openml.org/t/59\nEstimation Procedure.: crossvalidation\nEvaluation Measure...: predictive_accuracy\nTarget Feature.......: class\n# of Classes.........: 3\nCost Matrix..........: Available\n\n\n\n# Filtering for only task_id=59\ntask_df = task_df.query(\"task_id==59\")\n\n\n# Filtering based on Random Forest\ntask_rf =  task_df[task_df['flow_name'].str.contains(\"RandomForest\")]\ntask_rf.head()\n\n\n\n\n\n\n\n\nrun_id\ntask_id\nsetup_id\nflow_id\nflow_name\ndata_id\ndata_name\nfunction\nupload_time\nuploader\nuploader_name\nvalue\nvalues\narray_data\n\n\n\n\n144\n1849043\n59\n29015\n5500\nsklearn.ensemble.forest.RandomForestClassifier...\n61\niris\npredictive_accuracy\n2017-03-03 17:10:12\n1\njanvanrijn@gmail.com\n0.946667\nNone\nNone\n\n\n145\n1853409\n59\n30950\n5873\nsklearn.pipeline.Pipeline(Imputer=openml.utils...\n61\niris\npredictive_accuracy\n2017-03-21 22:08:01\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n146\n6130126\n59\n4163633\n7108\nsklearn.model_selection._search.RandomizedSear...\n61\niris\npredictive_accuracy\n2017-08-21 11:07:40\n1\njanvanrijn@gmail.com\n0.960000\nNone\nNone\n\n\n147\n6130128\n59\n4163634\n7108\nsklearn.model_selection._search.RandomizedSear...\n61\niris\npredictive_accuracy\n2017-08-21 11:08:06\n1\njanvanrijn@gmail.com\n0.946667\nNone\nNone\n\n\n190\n6946499\n59\n4978397\n7109\nsklearn.pipeline.Pipeline(imputation=openmlstu...\n61\niris\npredictive_accuracy\n2017-09-02 22:06:32\n1\njanvanrijn@gmail.com\n0.920000\nNone\nNone\n\n\n\n\n\n\n\n\n\nRetrieving top-performing models from OpenML\nSince we are an ambitious bunch of ML practitioners who settle for nothing but the best, and also since most results will not be considered worth the effort if not matching or beating state-of-the-art, we shall aim for the best scores. We’ll sort the filtered results we obtained based on the score or ‘value’ and then extract the components from that run - task and flow.\n\ntask_rf.sort_values(by='value', ascending=False).head()\n\n\n\n\n\n\n\n\nrun_id\ntask_id\nsetup_id\nflow_id\nflow_name\ndata_id\ndata_name\nfunction\nupload_time\nuploader\nuploader_name\nvalue\nvalues\narray_data\n\n\n\n\n3549\n523926\n59\n3526\n2629\nsklearn.ensemble.forest.RandomForestClassifier(8)\n61\niris\npredictive_accuracy\n2016-02-11 22:05:23\n869\np.gijsbers@student.tue.nl\n0.966667\nNone\nNone\n\n\n4353\n8955370\n59\n6890988\n7257\nsklearn.ensemble.forest.RandomForestClassifier...\n61\niris\npredictive_accuracy\n2018-04-06 16:32:22\n3964\nclear.tsai@gmail.com\n0.960000\nNone\nNone\n\n\n3587\n1852682\n59\n29263\n5500\nsklearn.ensemble.forest.RandomForestClassifier...\n61\niris\npredictive_accuracy\n2017-03-15 22:55:18\n1022\nrso@randalolson.com\n0.960000\nNone\nNone\n\n\n4375\n8886608\n59\n6835139\n7961\nsklearn.pipeline.Pipeline(Imputer=sklearn.prep...\n61\niris\npredictive_accuracy\n2018-03-17 16:46:27\n5032\nrashmi.kamath01@gmail.com\n0.960000\nNone\nNone\n\n\n3107\n1843272\n59\n24071\n4830\nsklearn.ensemble.forest.RandomForestClassifier...\n61\niris\npredictive_accuracy\n2016-12-08 20:10:03\n2\njoaquin.vanschoren@gmail.com\n0.960000\nNone\nNone\n\n\n\n\n\n\n\n\n# Fetching the Random Forest flow with the best score\nf = openml.flows.get_flow(2629)\nf\n\nOpenML Flow\n===========\nFlow ID.........: 2629 (version 8)\nFlow URL........: https://www.openml.org/f/2629\nFlow Name.......: sklearn.ensemble.forest.RandomForestClassifier\nFlow Description: Flow generated by openml_run\nUpload Date.....: 2016-02-11 21:17:08\nDependencies....: None\n\n\n\n# Fetching the run with the best score for\n# Random Forest on Iris\nr = openml.runs.get_run(523926)\nr\n\nOpenML Run\n==========\nUploader Name...: Pieter Gijsbers\nUploader Profile: https://www.openml.org/u/869\nMetric..........: predictive_accuracy\nResult..........: 0.966667\nRun ID..........: 523926\nRun URL.........: https://www.openml.org/r/523926\nTask ID.........: 59\nTask Type.......: Supervised Classification\nTask URL........: https://www.openml.org/t/59\nFlow ID.........: 2629\nFlow Name.......: sklearn.ensemble.forest.RandomForestClassifier(8)\nFlow URL........: https://www.openml.org/f/2629\nSetup ID........: 3526\nSetup String....: None\nDataset ID......: 61\nDataset URL.....: https://www.openml.org/d/61\n\n\nOkay, let’s take a pause and re-assess. From multiple users across the globe, who had uploaded runs to OpenML, for a Random Forest run on the Iris, the best score seen till now is 96.67%. That is certainly better than the naive model we built at the beginning to achieve 95.33%. We had used a basic 10-fold cross-validation to evaluate a Random Forest of 10 trees with a max depth of 2. Let us see, what the best run uses and if it differs from our approach.\n\n# The scoring metric used\nt.evaluation_measure\n\n'predictive_accuracy'\n\n\n\n# The methodology used for estimations\nt.estimation_procedure\n\n{'type': 'crossvalidation',\n 'parameters': {'number_repeats': '1',\n  'number_folds': '10',\n  'percentage': '',\n  'stratified_sampling': 'true'},\n 'data_splits_url': 'https://www.openml.org/api_splits/get/59/Task_59_splits.arff'}\n\n\n\n# The model used\nf.name\n\n'sklearn.ensemble.forest.RandomForestClassifier'\n\n\n\n# The model parameters\nfor param in r.parameter_settings:\n    name, value = param['oml:name'], param['oml:value']\n    print(\"{:&lt;25} : {:&lt;10}\".format(name, value))\n\nwarm_start                : False     \noob_score                 : False     \nn_jobs                    : 1         \nverbose                   : 0         \nmax_leaf_nodes            : None      \nbootstrap                 : True      \nmin_samples_leaf          : 1         \nn_estimators              : 10        \nmin_samples_split         : 2         \nmin_weight_fraction_leaf  : 0.0       \ncriterion                 : gini      \nrandom_state              : None      \nmax_features              : auto      \nmax_depth                 : None      \nclass_weight              : None      \n\n\nAs evident, our initial approach is different on two fronts. We didn’t explicitly use stratified sampling for our cross-validation. While the Random Forest hyperparameters are slightly different too (max_depth=None). That definitely sounds like a to-do, however, there is no reason why we should restrict ourselves to Random Forests. Remember, we are aiming big here. Given the number of OpenML users, there must be somebody who got a better score on Iris with some other model. Let us then retrieve that information. Programmatically, of course.\nIn summary, we are now going to sort the performance of all scikit-learn based models on Iris dataset as per the task definition with task_id=59.\n\n# Fetching top performances\ntask_df.sort_values(by='value', ascending=False).head()\n\n\n\n\n\n\n\n\nrun_id\ntask_id\nsetup_id\nflow_id\nflow_name\ndata_id\ndata_name\nfunction\nupload_time\nuploader\nuploader_name\nvalue\nvalues\narray_data\n\n\n\n\n3630\n2039748\n59\n180922\n6048\nsklearn.pipeline.Pipeline(dualimputer=helper.d...\n61\niris\npredictive_accuracy\n2017-04-09 01:09:01\n1104\njmapvhoof@gmail.com\n0.986667\nNone\nNone\n\n\n3631\n2039750\n59\n180924\n6048\nsklearn.pipeline.Pipeline(dualimputer=helper.d...\n61\niris\npredictive_accuracy\n2017-04-09 01:17:39\n1104\njmapvhoof@gmail.com\n0.986667\nNone\nNone\n\n\n3624\n2012939\n59\n157622\n6048\nsklearn.pipeline.Pipeline(dualimputer=helper.d...\n61\niris\npredictive_accuracy\n2017-04-06 23:29:28\n1104\njmapvhoof@gmail.com\n0.986667\nNone\nNone\n\n\n3618\n2012930\n59\n157613\n6048\nsklearn.pipeline.Pipeline(dualimputer=helper.d...\n61\niris\npredictive_accuracy\n2017-04-06 23:00:24\n1104\njmapvhoof@gmail.com\n0.986667\nNone\nNone\n\n\n3626\n2012941\n59\n157624\n6048\nsklearn.pipeline.Pipeline(dualimputer=helper.d...\n61\niris\npredictive_accuracy\n2017-04-07 01:36:00\n1104\njmapvhoof@gmail.com\n0.986667\nNone\nNone\n\n\n\n\n\n\n\n\n# Fetching best performing flow\nf = openml.flows.get_flow(6048)\nf\n\nOpenML Flow\n===========\nFlow ID.........: 6048 (version 1)\nFlow URL........: https://www.openml.org/f/6048\nFlow Name.......: sklearn.pipeline.Pipeline(dualimputer=helper.dual_imputer.DualImputer,nusvc=sklearn.svm.classes.NuSVC)\nFlow Description: Automatically created scikit-learn flow.\nUpload Date.....: 2017-04-06 22:42:59\nDependencies....: sklearn==0.18.1\nnumpy&gt;=1.6.1\nscipy&gt;=0.9\n\n\n\n# Fetching best performing run\nr = openml.runs.get_run(2012943)\n\n# The model parameters\nfor param in r.parameter_settings:\n    name, value = param['oml:name'], param['oml:value']\n    print(\"{:&lt;25} : {:&lt;10}\".format(name, value))\n\nsteps                     : [('DualImputer', &lt;helper.dual_imputer.DualImputer object at 0x7ff618e4d908&gt;), ('nusvc', NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n   max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True,\n   tol=3.2419092644286417e-05, verbose=False))]\ncache_size                : 200       \nclass_weight              : None      \ncoef0                     : 0.0       \ndecision_function_shape   : None      \ndegree                    : 3         \ngamma                     : auto      \nkernel                    : linear    \nmax_iter                  : -1        \nnu                        : 0.3       \nprobability               : True      \nrandom_state              : 3         \nshrinking                 : True      \ntol                       : 3.24190926443e-05\nverbose                   : False     \n\n\nThe highest score obtained among the uploaded results is 98.67% using a variant of SVM. However, if we check the corresponding flow description, we see that it is using an old scikit-learn version (0.18.1) and therefore may not be possible to replicate the exact results. However, in order to improve from our score of 95.33%, we should try running a nu-SVC on the same problem and see where we stand. Let’s go for it. Via OpenML, of course.\n\n\nRunning best performing flow on the required task\n\nimport openml\nimport numpy as np\nfrom sklearn.svm import NuSVC\n\n\n# Building the NuSVC model object with parameters found\nclf = NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n   decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n   max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True,\n   tol=3.2419092644286417e-05, verbose=False)\n\n\n# Obtaining task used earlier\nt = openml.tasks.get_task(59)\nt\n\nOpenML Classification Task\n==========================\nTask Type Description: https://www.openml.org/tt/1\nTask ID..............: 59\nTask URL.............: https://www.openml.org/t/59\nEstimation Procedure.: crossvalidation\nEvaluation Measure...: predictive_accuracy\nTarget Feature.......: class\n# of Classes.........: 3\nCost Matrix..........: Available\n\n\n\n# Running the model on the task\n# Internally, the model will be made into \n# an OpenML flow and we can choose to retrieve it\nr, f = openml.runs.run_model_on_task(model=clf, task=t, upload_flow=False, return_flow=True)\nf\n\nOpenML Flow\n===========\nFlow Name.......: sklearn.svm.classes.NuSVC\nFlow Description: Nu-Support Vector Classification.\n\nSimilar to SVC but uses a parameter to control the number of support\nvectors.\n\nThe implementation is based on libsvm.\nDependencies....: sklearn==0.21.3\nnumpy&gt;=1.6.1\nscipy&gt;=0.9\n\n\n\n# To obtain the score (without uploading)\n## r.publish() can be used to upload these results\n## need to sign-in to https://www.openml.org/\nscore = []\nevaluations = r.fold_evaluations['predictive_accuracy'][0]\nfor key in evaluations:\n    score.append(evaluations[key])\nprint(np.mean(score))\n\n0.9866666666666667\n\n\nLo and behold! We hit the magic number. I personally would have never tried out NuSVC and would have stuck around tweaking hyperparameters of the Random Forest. This is a new discovery of sorts for sure. I wonder though if anybody has tried XGBoost on Iris?\nIn any case, we can now upload the results of this run to OpenML using:\n\nr.publish()\n\nOpenML Run\n==========\nUploader Name: None\nMetric.......: None\nRun ID.......: 10464835\nRun URL......: https://www.openml.org/r/10464835\nTask ID......: 59\nTask Type....: None\nTask URL.....: https://www.openml.org/t/59\nFlow ID......: 18579\nFlow Name....: sklearn.svm.classes.NuSVC\nFlow URL.....: https://www.openml.org/f/18579\nSetup ID.....: None\nSetup String.: Python_3.6.9. Sklearn_0.21.3. NumPy_1.16.4. SciPy_1.4.1. NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n      decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n      max_iter=-1, nu=0.3, probability=True, random_state=3, shrinking=True,\n      tol=3.241909264428642e-05, verbose=False)\nDataset ID...: 61\nDataset URL..: https://www.openml.org/d/61\n\n\nOne would need to sign-in to https://www.openml.org/ and generate their respective apikey. The results would then be available for everyone to view and who knows, you can have your name against the best-ever performance measured on the Iris dataset!\n\nThis post was in no ways intended to be a be-all-end-all guide to OpenML. The primary goal was to help form an acquaintance with the OpenML terminologies, introduce the API, establish connections with the general ML practices, and give a sneak-peek into the potential benefits of working together as a community. For a better understanding of OpenML, please explore the documentation. If one desires to continue from the examples given in this post and explore further, kindly refer to the API.\nOpenML-Python is an open-source project and contributions from everyone in the form of Issues and Pull Requests are most welcome. Contribution to the OpenML community is in fact not limited to code contribution. Every single user can make the community richer by sharing data, experiments, results, using OpenML.\nAs ML practitioners, we may be dependent on tools for our tasks. However, as a collective, we can juice out its potential to a larger extent. Let us together, make ML more transparent, more democratic!\n\nSpecial thanks to Heidi, Bilge, Sahithya, Matthias, Ashwin for the ideas, feedback, and support.\n\nRelated readings: * To get started with OpenML-Python * OpenML-Python Github * The OpenML website * Miscellaneous reading on OpenML * To get in touch!"
  },
  {
    "objectID": "posts/2016-09-18-mlr-loves-OpenML/2016-09-18-mlr-loves-OpenML.html",
    "href": "posts/2016-09-18-mlr-loves-OpenML/2016-09-18-mlr-loves-OpenML.html",
    "title": "mlr loves OpenML",
    "section": "",
    "text": "OpenML stands for Open Machine Learning and is an online platform, which aims at supporting collaborative machine learning online. It is an Open Science project that allows its users to share data, code and machine learning experiments.\nAt the time of writing this blog post I am in Eindhoven at an OpenML workshop, where developers and scientists meet to work on improving the project. Some of these people are R users and they (we) are developing an R package that communicates with the OpenML platform.\n\n\nOpenML in R\nThe OpenML R package can list and download data sets and machine learning tasks (prediction challenges). In R one can run algorithms on the these data sets/tasks and then upload the results to OpenML. After successful uploading, the website shows how well the algorithm performs. To run the algorithm on a given task the OpenML R package builds on the mlr package. mlr understands what a task is and can run learners on that task. So all the OpenML package needs to do is convert the OpenML objects to objects mlr understands and then mlr deals with the learning.\n\n\nA small case study\nWe want to create a little study on the OpenML website, in which we compare different types of Support Vector Machines. The study gets an ID assigned to it, which in our case is 27. We use the function ksvm (with different settings of the function argument type) from package kernlab, which is integrated in mlr (“classif.ksvm”).\n\nFor details on installing and setting up the OpenML R package please see the guide on GitHub.\nLet’s start conducting the study:\n\nLoad the packages and list all tasks which have between 100 and 500 observations.\n\nlibrary(\"OpenML\") library(\"mlr\") library(\"farff\") library(\"BBmisc\")\n\ndsize = c(100, 500) taskinfo_all = listOMLTasks(number.of.instances = dsize)\n\nSelect all supervised classification tasks that do 10-fold cross-validation and choose only one task per data set. To keep the study simple and fast to compute, select only the first three tasks.\n\ntaskinfo_10cv = subset(taskinfo_all,\n     task.type == \"Supervised Classification\" &\n     estimation.procedure == \"10-fold Crossvalidation\" &\n     evaluation.measures == \"predictive_accuracy\" &\n     number.of.missing.values == 0 &\n     number.of.classes %in% c(2, 4))\n\ntaskinfo = taskinfo_10cv[1:3, ]\n\nCreate the learners we want to compare.\n\nlrn.list = list(\n   makeLearner(\"classif.ksvm\", type = \"C-svc\"),\n   makeLearner(\"classif.ksvm\", type = \"kbb-svc\"),\n   makeLearner(\"classif.ksvm\", type = \"spoc-svc\")\n)\n\nRun the learners on the three tasks.\n\ngrid = expand.grid(task.id = taskinfo$task.id,\n                   lrn.ind = seq_along(lrn.list))\n\nruns = lapply(seq_row(grid), function(i) {\n  message(i)\n  task = getOMLTask(grid$task.id[i])\n  ind = grid$lrn.ind[i]\n  runTaskMlr(task, lrn.list[[ind]])\n})\n\nAnd finally upload the runs to OpenML. The upload function (uploadOMLRun) returns the ID of the uploaded run object. When uploading runs that are part of a certain study, tag it with study_ and the study ID. After uploading the runs appear on the website and can be found using the tag or via the study homepage.\n\n## please do not spam the OpenML server by uploading these\n## tasks. I already did that.\nrun.id = lapply(runs, uploadOMLRun, tags = \"study_27\")\n\nTo show the results of our study, list the run evaluations and make a nice plot.\n\nevals = listOMLRunEvaluations(tag = \"study_27\")\n\nevals$task.id = as.factor(evals$task.id)\nevals$setup.id = as.factor(evals$setup.id)\n\nlibrary(\"ggplot2\")\nggplot(evals, aes(x = setup.id, y = predictive.accuracy,\n                  color = data.name, group = task.id)) +\n  geom_point() + geom_line()\n\nNow you can go ahead and create a bigger study using the techniques you have learned.\n\n\nFurther infos\nIf you are interested in more, check out the OpenML blog, the paper and the GitHub repos.\n\nOriginally published at mlr-org.github.io."
  },
  {
    "objectID": "posts/2020-05-06-Reproducible-deep-learning-with-OpenML/2020-05-06-Reproducible-deep-learning-with-OpenML.html",
    "href": "posts/2020-05-06-Reproducible-deep-learning-with-OpenML/2020-05-06-Reproducible-deep-learning-with-OpenML.html",
    "title": "Reproducible deep learning with OpenML",
    "section": "",
    "text": "Deep learning is facing a reproducibility crisis right now[1]. The scale of experiments and there are numerous hyperparameters that affect performance, which makes it hard for the author to write a reproducibility document. The current best way to make an experiment reproducible is to upload the code. However, that’s not optimal in a lot of situations where we have a huge undocumented codebase and someone would like to just reproduce the model. OpenML[2] is an online machine learning platform for sharing and organizing data, machine learning algorithms and experiments. Until now we only provided support for classical machine learning and libraries like Sklearn and MLR. We see there is a huge need for reproducible deep learning now. To solve this issue OpenML is launching its deep learning plugins for popular deep learning libraries like Keras, MXNet, and Pytorch.\nHere we have a small tutorial on how to use our pytorch extension with MNIST dataset.\nSetup To install openml and openml pytorch extension execute this instruction in your terminal  pip install openml openml_pytorch\n\n!pip install openml openml_pytorch\n\nCollecting openml\n  Downloading https://files.pythonhosted.org/packages/68/5b/cd32bb85651eccebfb489cc6ef7f060ce0f62350a6239127e398313090cc/openml-0.10.2.tar.gz (158kB)\n\n     |██                              | 10kB 28.5MB/s eta 0:00:01\n     |████▏                           | 20kB 6.1MB/s eta 0:00:01\n     |██████▏                         | 30kB 8.6MB/s eta 0:00:01\n     |████████▎                       | 40kB 10.9MB/s eta 0:00:01\n     |██████████▎                     | 51kB 7.2MB/s eta 0:00:01\n     |████████████▍                   | 61kB 8.4MB/s eta 0:00:01\n     |██████████████▍                 | 71kB 9.6MB/s eta 0:00:01\n     |████████████████▌               | 81kB 10.7MB/s eta 0:00:01\n     |██████████████████▌             | 92kB 8.5MB/s eta 0:00:01\n     |████████████████████▋           | 102kB 9.3MB/s eta 0:00:01\n     |██████████████████████▊         | 112kB 9.3MB/s eta 0:00:01\n     |████████████████████████▊       | 122kB 9.3MB/s eta 0:00:01\n     |██████████████████████████▉     | 133kB 9.3MB/s eta 0:00:01\n     |████████████████████████████▉   | 143kB 9.3MB/s eta 0:00:01\n     |███████████████████████████████ | 153kB 9.3MB/s eta 0:00:01\n     |████████████████████████████████| 163kB 9.3MB/s \nCollecting openml_pytorch\n  Downloading https://files.pythonhosted.org/packages/5b/a4/8c69a041e7929d93460db17cf276abfb7b49af9c3d5077bee1c52101ba4c/openml_pytorch-0.0.1-py3-none-any.whl\nCollecting liac-arff&gt;=2.4.0\n  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\nCollecting xmltodict\n  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\nRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from openml) (2.21.0)\nRequirement already satisfied: scikit-learn&gt;=0.18 in /usr/local/lib/python3.6/dist-packages (from openml) (0.22.2.post1)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from openml) (2.8.1)\nRequirement already satisfied: pandas&gt;=0.19.2 in /usr/local/lib/python3.6/dist-packages (from openml) (1.0.3)\nRequirement already satisfied: scipy&gt;=0.13.3 in /usr/local/lib/python3.6/dist-packages (from openml) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.6.2 in /usr/local/lib/python3.6/dist-packages (from openml) (1.18.2)\nCollecting torch==1.2.0\n  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n     |████████████████████████████████| 748.9MB 14kB/s \nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (3.0.4)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (1.24.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (2020.4.5.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;openml) (2.8)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn&gt;=0.18-&gt;openml) (0.14.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil-&gt;openml) (1.12.0)\nRequirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.19.2-&gt;openml) (2018.9)\nBuilding wheels for collected packages: openml, liac-arff\n  Building wheel for openml (setup.py) ... done\n  Created wheel for openml: filename=openml-0.10.2-cp36-none-any.whl size=190318 sha256=50c2fd823d13904f246bf30997a2464379c393377cfd77f74b5dace4935db99c\n  Stored in directory: /root/.cache/pip/wheels/71/ec/5f/aaad9e184680b0b8f1a02ff0ec640cace5adf5bff7bb0af1b4\n  Building wheel for liac-arff (setup.py) ... done\n  Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13335 sha256=fca5bc5e07e3fe4f591cbe79968a42e43f535b3bd3e4be77b62c901d46feaaa9\n  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\nSuccessfully built openml liac-arff\nERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 1.2.0 which is incompatible.\nInstalling collected packages: liac-arff, xmltodict, openml, torch, openml-pytorch\n  Found existing installation: torch 1.4.0\n    Uninstalling torch-1.4.0:\n      Successfully uninstalled torch-1.4.0\nSuccessfully installed liac-arff-2.4.0 openml-0.10.2 openml-pytorch-0.0.1 torch-1.2.0 xmltodict-0.12.0\n\n\nLet’s import the necessary libraries\n\nimport torch.nn\nimport torch.optim\nimport openml\nimport openml_pytorch\n\nimport logging\n\nSet the apikey for openml python library, you can find your api key in your openml.org account\n\nopenml.config.apikey = 'key'\n\nDefine a sequential network that does initial image reshaping and normalization model\n\nprocessing_net = torch.nn.Sequential(\n    openml_pytorch.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 1, 28, 28)),\n    torch.nn.BatchNorm2d(num_features=1)\n)\nprint(processing_net)\n\nSequential(\n  (0): Functional()\n  (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)\n\n\nDefine a sequential network that does the extracts the features from the image.\n\nfeatures_net = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n)\nprint(features_net)\n\nSequential(\n  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n  (1): LeakyReLU(negative_slope=0.01)\n  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n  (4): LeakyReLU(negative_slope=0.01)\n  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nDefine a sequential network that flattens the features and compiles the results into probabilities for each digit.\n\nresults_net = torch.nn.Sequential(\n    openml_pytorch.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 4 * 4 * 64)),\n    torch.nn.Linear(in_features=4 * 4 * 64, out_features=256),\n    torch.nn.LeakyReLU(),\n    torch.nn.Dropout(),\n    torch.nn.Linear(in_features=256, out_features=10),\n)\nprint(results_net)\n\nSequential(\n  (0): Functional()\n  (1): Linear(in_features=1024, out_features=256, bias=True)\n  (2): LeakyReLU(negative_slope=0.01)\n  (3): Dropout(p=0.5, inplace=False)\n  (4): Linear(in_features=256, out_features=10, bias=True)\n)\n\n\nThe main network, composed of the above specified networks.\n\nmodel = torch.nn.Sequential(\n    processing_net,\n    features_net,\n    results_net\n)\nprint(model)\n\nSequential(\n  (0): Sequential(\n    (0): Functional()\n    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (1): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n    (1): LeakyReLU(negative_slope=0.01)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n    (4): LeakyReLU(negative_slope=0.01)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (2): Sequential(\n    (0): Functional()\n    (1): Linear(in_features=1024, out_features=256, bias=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=256, out_features=10, bias=True)\n  )\n)\n\n\nDownload the OpenML task for the mnist 784 dataset.\n\ntask = openml.tasks.get_task(3573)\n\nRun the model on the task and publish the results on openml.org\n\n\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n\nrun.publish()\n\nprint('URL for run: %s/run/%d' % (openml.config.server, run.run_id))\n\nURL for run: https://www.openml.org/api/v1/xml/run/10452577\n\n\nBy going to the published URL you can check the model performance and other metadata\n\nWe hope that openml deep learning plugins can help in reproducing deep learning experiments and provide a universal reproducibility platform for the experiments. Here are the links of all supported deep learning plugins right now:\n\nMXNet: https://github.com/openml/openml-mxnet\nKeras: https://github.com/openml/openml-keras\nPytorch: https://github.com/openml/openml-pytorch\nONNX: https://github.com/openml/openml-onnx\n\nThere are examples of how to use these libraries in the Github repos. These libraries are in the development stage right now so we would appreciate any feedback on Github issues of these libraries. Links:\n\nhttps://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/\nhttps://www.openml.org"
  },
  {
    "objectID": "posts/2017-03-03-Basic-components-of-OpenML/2017-03-03-Basic-components-of-OpenML.html",
    "href": "posts/2017-03-03-Basic-components-of-OpenML/2017-03-03-Basic-components-of-OpenML.html",
    "title": "Basic components of OpenML",
    "section": "",
    "text": "During my PhD, we developed OpenML, an online experiment database for Machine Learning. Researchers are encouraged to upload their experimental results on it, so that these can be reused by anyone. Various high level papers have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013, SIGKDD Explorations and JLMR). However, there is no clear overview of the basic components upon which the platform is build. In this blog post I will review these, and discuss some best practises.\n\nData\nOne of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset. It shows all features, once of these is identified as the ‘default target attribute’, although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has it’s own unique ID. Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions:\n\nGet all available datasets\nGet dataset (required the data id)\nGet data features (requires the data id)\nGet data qualities (requires the data id)\n\n\n\nTask types and tasks\nA dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one. Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds. Useful API operations include:\n\nGet all available tasks\nGet all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type)\nGet the details of a task (requires task id)\n\nCurrently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us.\n\n\nFlows\nTasks can be ‘solved’ by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow ‘Decision Tree’ which would make the flow ‘Bagging of Decision Trees’. A flow is distinguished by it’s name and ‘external version’, which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR). Useful API functions are:\n\nList all flows\nList all my flows\nGive details about a given flow(requires flow id)\n\n\n\nRuns\nWhenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers resuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided. Some useful API functions:\n\nList all runs performed on a given task (requires task id, e.g., the iris task is 59)\nCompare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree)\nAnd many more …\n\nUsually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post.\n\n\nSetups\nEvery run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings. For example,\n\nCompare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations)\n\nAs setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. A later blogpost will detail on these."
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "",
    "text": "Twice a year the OpenML community organizes an OpenML workshop. Next week we will kick off our fall workshop, which will be hosted virtually and is open to everyone! At the workshop we work on the development and maintenance of the OpenML platform (website, APIs and extensions), but also encourage discussions on a broad range of topics, from using the platform to building the platform to building the community. If you are already sure you want to join, please register and we will see you next week!\nThe workshops are the perfect time to get involved with the OpenML community. We welcome first time contributors to the project. You can contribute in many ways, such as giving feedback, cleaning data, or helping with design, planning, or content. If you are looking to contribute to the software projects, we are there to help you get started on beginner issues. Even better, the workshop coincides with Hacktoberfest, which means you can earn a shirt through your open source software contributions! To get an even better idea of what happens at an OpenML workshop, read the write-up of last year’s fall workshop and look at the workshop agenda.\nIn this blog post we will go over the basics on how you can participate in the workshop. This includes pointers to our code repositories, the platform we will use (gather.town), and participating in Hacktoberfest."
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#overview",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#overview",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "Overview",
    "text": "Overview\nWhere? openml at gather.town\nWhen? 26th through 30th of October 2020, 09.00-17.00 CET.\nWhat? Discussions about OpenML, contributing to any OpenML project, or experimenting with OpenML.\nPlease register if you plan to join. It is not required to attend the workshop in full. You are free to join only for a day or specific discussions.\nHowever, it is helpful to indicate which meetings you want to attend, so we know who to expect.\nThe workshop agenda is public. If you see a meeting which you would like to join, just add your name to the list. Feel free to propose a new meeting by adding a proposal to the meeting list!"
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#gather-town",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#gather-town",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "Gather Town",
    "text": "Gather Town\nThis fall, we will host the OpenML Hackathon at our gather.town space. Gather.town is a web-based platform that allows you to video conference with other people, but with a twist. Rather than the conventional conference call, in gather.town you control an avatar that can navigate a virtual world. You will automatically be connected to anyone that is within the vicinity of your avatar. This means you can more organically join groups of people, or walk from one meeting room to the next. Here is a preview of our OpenML map: \nAt a glance you see two types of areas: those marked with purple and those that are not marked. Areas that are not marked (e.g. the main lobby) function exactly as described above. However, whenever you are in a purple area you will automatically be connected to everyone else in the same purple area. So whenever you are interested in joining people discussing the new openml website, you just head over to that room!"
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#breakout-sessions",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#breakout-sessions",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "Breakout Sessions",
    "text": "Breakout Sessions\nIn the workshop agenda you will find various breakout sessions. These cover a wide range of topics, and we will briefly cover a few of them to give you an impression.\n\nThe Dataset Quality break-out session discusses what quality standards we want to uphold for datasets on OpenML. Ideally, we would like all our datasets perfectly annotated with a description of how the data was collected, what each features means, and what the feature types are. In reality, datasets are collected from different sources and often uploaded by someone other than the original creator. This leads to issues such as incomplete data, badly annotated columns, or multiple uploads of the same data.How can we ensure that despite these practical issues, it remains easy to find datasets of interest on OpenML? How can we improve the dataset quality of datasets uploaded to OpenML? How can we set these new standards and guidelines in a maintainable way? How can we automate annotation of attribute types to the dataset? How can we detect corrupt datasets before uploading it to server?\nThe Website Feedback break-out session is to discuss the newly designed OpenML website. We will appreciate any feedback on frontend, functionalities and limitations of the new openml website.\nWe recently discussed that OpenML wants to improve support for a wider range of datasets. For this reason we are moving away from the ARFF format. The Dataset Format break-out session discusses this transition. Our current plan is to migrate to parquet format, we will be discussing about limitations and advantages of new format and how can we integrate parquet to OpenML.\n\nWe welcome your participation in any these sessions.\nAnd if you have an interesting topic we should discuss, feel free to submit your own breakout proposal!"
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#hacktoberfest",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#hacktoberfest",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "Hacktoberfest",
    "text": "Hacktoberfest\nHacktoberfest is a global event to support open source software. If you contribute “enough” to open source projects, you will also be eligible to receive a free t-shirt and some other swag! Our repositories welcome Hacktoberfest participation, so whenever you contribute to our repositories, you are working towards your Hacktoberfest goal. The rules are simple: - Sign up at Hacktoberfest with Github credentials - Find an issue you can help with (more on that below) - Submit a pull request resolving the issue - To be eligible for the t-shirt and swag, you must open four pull requests (this is the “enough” part)\n\nYou can browse our repositories and identify issues which you would like to work on. To make things easier, here are the list of openml repositories that are extra prepared for Hacktoberfest. These repositories have core developers registered to attend the workshop, and have marked issues which are suitable for newcomers with the hacktoberfest label.\n\nopenml.org The front-end for the new OpenML! (python, React.js)\nOpenML The back-end for OpenML. (php)\nopenml-python The Python API for interacting with the OpenML server. Please read this welcome message to get started. (python)\ndocs Documentation for OpenML.\n\nPlease be sure to check the respective repositories contribution guidelines, and don’t be afraid to ask questions! We were all new to open source once and are happy to help you get started too. If you are just getting your feet wet with open source contribution, a great way to get started it to just try and use the packages/website. If you see documentation which needs clarification, or perhaps even just a simple typo, we appreciate your help in fixing it!"
  },
  {
    "objectID": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#we-hope-to-see-you-at-our-workshop",
    "href": "posts/OpenML-Hacktoberfest/2020-10-23-OpenML-Hacktoberfest.html#we-hope-to-see-you-at-our-workshop",
    "title": "Participate in the OpenML Hacktoberfest Workshop!",
    "section": "We hope to see you at our workshop!",
    "text": "We hope to see you at our workshop!\nIf you have any questions about the workshop, feel free to leave a comment."
  },
  {
    "objectID": "posts/2016-05-02-OpenML/2016-05-02-OpenML.html",
    "href": "posts/2016-05-02-OpenML/2016-05-02-OpenML.html",
    "title": "OpenML",
    "section": "",
    "text": "OpenML is a very cool new online platform that aims at improving — as the name says — Open Machine Learning. It stands for Open Data, Open Algorithms and Open Research. OpenML is still in it’s beta phase, but already pretty awesome.\nWith this blog post I would like to introduce the main concepts, show who should be interested in the platform and I will go a little into a challenge it faces.\n\nConcepts\nThe following four concepts form the basis of the platform:\n\ndata\ntask\nflow\nrun\n\nThe figure shows how they are connected.\n\n\n\nWho can make use of OpenML?\n\nThe domain scientist\nYou have data that you do not know how to analyse best? Upload your data to OpenML and you will have the whole world helping you. Write a good data and task description to make sure people understand the problem.\n\n\nThe data analyst\nYou like taking part in challenges? Being the best solver of a task? Go to OpenML and check out the many tasks and go solve!\n\n\nThe algorithm developer\nYou developed a statistical method or a machine learning algorithm and want to try it out? You will find plenty of data sets and the possibility to make your algorithm public.\n\n\nThe student\nYou study statistics, data science, machine learning? You want to know what is out there? On OpenML you will find a wide variety of algorithms and, if the solvers do a good job, info on software and implementation.\n\n\nThe teacher\nYou teach a machine learning class and want the students to participate in a challenge? Make up your own task and let the students try solving it. The platform shows who uploaded what and when.\n\n\nThe unknown\nThere are possibly many other people who will benefit from the platform, like meta analysts, benchmarkers and people I can not think of right now.\n\n\n\nHow to use OpenML\nOther than just browsing the website you can access OpenML through quite some interfaces such as R or WEKA. For an example on how to use the R interface check out the tutorial.\nThe whole project is of course open source. Check out the different git repositories for all the code and in case you have any complaints.\n\n\nThe overfitting problem\nPlatforms like kaggle, crowdanalytics and innocentive host challenges and give people only part of the data so they can evaluate the performance of the algorithm on a separate data set to (try to) prevent overfitting. So far OpenML does not do that. It always shows all the data, and algorithms are evaluated via resampling procedures (on OpenML called estimation procedures). There are big discussions about how to solve the problem of overfitting on OpenML. They go from keeping part of the data hidden for a certain time in the beginning to doing repeated cross-validation on the (overly) good performing flows on a given task. If you have ideas here, please don’t hesitate to leave me a comment.\nThe platform is still in it’s childhood and may not be perfect yet (If you find issues, post them on the github page). But I think it can grow to be a great thing one day.\n\nOriginally published at heidiseibold.github.io on May 2, 2016."
  },
  {
    "objectID": "posts/2020-03-23-Finding a-standard-dataset-format-for-machine-learning/2020-03-23-Finding a-standard-dataset-format-for-machine-learning.html",
    "href": "posts/2020-03-23-Finding a-standard-dataset-format-for-machine-learning/2020-03-23-Finding a-standard-dataset-format-for-machine-learning.html",
    "title": "Finding a standard dataset format for machine learning",
    "section": "",
    "text": "Machine learning data is commonly shared in whatever form it comes in (e.g. images, logs, tables) without being able to make strict assumptions on what it contains or how it is formatted. This makes machine learning hard because you need to spend a lot of time figuring out how to parse and deal with it. Some datasets are accompanied with loading scripts, which are language-specific and may break, and some come with their own server to query the dataset. These do help, but are often not available, and still require us to handle every dataset individually.\nWith OpenML, we aim to take a stress-free, 'zen'-like approach to working with machine learning datasets. To make training data easy to use, OpenML serves thousands of datasets in the same format, with the same rich meta-data, so that you can directly load it (e.g. in numpy,pandas,…) and start building models without manual intervention. For instance, you can benchmark algorithms across hundreds of datasets in a simple loop.\nFor historical reasons, we have done this by internally storing all data in the ARFF data format, a CSV-like text-based format that includes meta-data such as the correct feature data types. However, this format is loosely defined, causing different parsers to behave differently, and the current parsers are memory-inefficient which inhibits the use of large datasets. A more popular format these days is Parquet, a binary single-table format. However, many current machine learning tasks require multi-table data. For instance, image segmentation or object detection tasks have both images and varying amounts of annotations per image.\nIn short, we are looking the best format to internally store machine learning datasets in the foreseeable future, to extend OpenML towards all kinds of modern machine learning datasets and serve them in a uniform way. This blog post presents out process and insights. We would love to hear your thoughts and experiences before we make any decision on how to move forward.\nScope\nWe first define the general scope of the usage of the format:\n\nIt should be useful for data storage and transmission. We can always convert data during upload or download in OpenML's client APIs. For instance, people may upload a Python pandas dataframe to OpenML, and later get the same dataframe back, without realizing or caring how the data was stored in the meantime. If people want to store the data locally, they can download it in the format they like (e.g. a memory-mapped format like Arrow/Feather for fast reading or TFRecords for people using TensorFlow). Additional code can facilitate such conversions.\nThere should be a standard way to represent specific types of data, i.e. a fixed schema that can be verified. For instance, all tabular data should be stored in a uniform way. Without it, we would need dataset-specific code for loading, which requires maintenance, and it will be harder to check quality and extract meta-data.\nThe format should allow storing most (processed) machine learning datasets, including images, video, audio, text, graphs, and multi-tabular data such as object recognition tasks and relational data. Data such as images can be converted to numeric formats (e.g. pixel values) for storage in this format (and usage in machine learning).\n\nImpact on OpenML (simplicity, maintenance)\nSince OpenML is a community project, we want to keep it as easy as possible to use and maintain:\n\nWe aim to host datasets in an S3 object store (min.io).\nWe prefer a single internal data format to reduce maintenance both server-side and client-side.\nWe need machine-readable schemas (in a specific language) that describe how a certain type of data is formatted. Examples would be a schema for tabular data, a schema for annotated image data, etc. Every dataset should specify the schema it satisfies, and we should be able to validate this. We aim to gradually roll out support form different types of data, starting with tabular, and including others only after schemas are defined.\nWe need to support batch data now, but ideally the format should allow data appending (streaming) in the future.\n\nWhen no agreed upon schema exists, we could offer a forum for the community to discuss and agree on a standard schema, in collaboration with other initiatives (e.g. frictionlessdata). For instance, new schemas could be created in a github repo to allow people to do create pull requests. They could be effectively used once they are merged.\nRequirements\nTo draw up a shortlist of data formats, we used the following (soft) requirements:\n\nThe format should be stable and fully maintained by an active community.\nParsers in various programming languages, including well-maintained and stable libraries.\nStreaming read/writes, for easy conversion and memory efficiency.\nVersion control, some way to see differences between versions.\nIdeally, there is a way to detect bitflip errors during storage or transmission.\nIdeally, fast read/writes and efficient storage.\nIdeally, there should be support for storing sparse data.\nSupport for storing binary blobs and vectors of different lengths.\nIf possible, support for multiple ‘resources’ (e.g. collections of files or multiple relational tables).\nPotentially, store some meta-data inside the file.\n\nShortlist\nWe decided to investigate the following formats in more detail:\nArrow / Feather\nBenefits:\n\nGreat for locally caching files after download\nMemory-mapped, so very fast reads\n\nDrawbacks:\n\nNot stable enough yet and not ideal for long-term storage. The authors also discourage it for long-term storage.\nLimited to one data structure per file, but that data structure can be complex (e.g. dict).\n\nParquet\nBenefits:\n\nUsed in industry a lot, active developer community. Good community of practice.\nWell-supported and maintained.\nHas parsers in different languages, but not all Parquet features are supported in every library (see below).\nBuilt-in compression (columnar storage), very efficient long-term data storage\nSimple structure\nSparse data\n\nDrawbacks:\n\nThe Python libraries (Arrow, fastparquet) do not support partial read/writes. The Java/Go implementations do. Splitting up parquet files into many small files can be cumbersome.\nNo version control, no meta-data storage, no schema enforcement. There are layers on top (e.g. delta lake) that do support this. Simple file versioning can also be done with S3.\nThe different parsers (e.g. Parquet support inside Arrow, fastparquet) implement different parts of the Parquet format and different set of compression algorithms. Hence, parquet files may not be compatible between parsers (see here and here.\nSupport limited to single-table storage. For instance, there doesn’t seem to be an apparent way to store an object detection dataset (with images and annotations) as a single parquet file.\n\nSQLite\nBenefits:\n\nEasy to use and comparably fast to HDF5 in our tests.\nVery good support in all languages. It is built-in in Python.\nVery flexible access to parts of the data. SQL queries can be used to select any subset of the data.\n\nDrawback:\n\nIt supports only 2000 columns, and we have quite a few datasets with more than 2000 features. Hence, storing large tabular data will require mapping data differently, which would add a lot of additional complexity.\nWriting SQL queries requires knowledge of the internal data structure (tables, keys,…).\n\nHDF5\nBenefits:\n\nVery good support in all languages. Has well-tested parsers, all using the same C implementation.\nWidely accepted format in the deep learning community to store data and models.\nWidely accepted format in many scientific domains (e.g. astronomy, bioinformatics,…)\nProvides built-in compression. Constructing and loading datasets was reasonably fast.\nVery flexible. Should allow to store any machine learning dataset as a single file.\nAllows easy inclusion of meta-data inside the file, creating a self-contained file.\nSelf-descriptive: the structure of the data can be easily read programmatically. For instance, ‘dump -H -A 0 mydata.hdf5’ will give you a lot of detail on the structure of the dataset.\n\nDrawbacks:\n\nComplexity. We cannot make any a priori assumptions about how the data is structured. We need to define schema and implement code that automatically validates that a dataset follows a specific schema (e.g. using h5dump to see whether it holds a single dataframe that we could load into pandas). We are unaware of any initiatives to define such schema.\nThe format has a very long and detailed specification. While parsers exist we don’t really know whether they are fully compatible with each other.\nCan become corrupt if not carefully used.\n\nCSV\nBenefits:\n\nVery good support in all languages.\nEasy to use, requires very little additional tooling\nText-based, so easy versioning with git LFS. Changes in different versions can be observed with a simple git diff.\nThe current standard used in frictionlessdata.\nThere exist schema to express certain types of data in CSV (see frictionlessdata).\n\nDrawbacks:\n\nNot very efficient for storing floating point numbers\nNot ideal for very large datasets (when data does not fit in memory/disk)\nMany different dialects exist. We need to decide on a standardized dialect and enforce that only that dialect is used on OpenML (https://frictionlessdata.io/specs/csv-dialect/). The dialect specified in RFC4180, which uses the comma as delimiter and the quotation mark as quote character, is often recommended.\n\nOverview\n\n\n\n\nParquet\nHDF5\nSQLite\nCSV\n\n\n\n\nConsistency across different platforms\n?\n✅\n✅\n✅ (dialect)\n\n\nSupport and documentation\n✅\n✅\n✅\n✅\n\n\nRead/write speed\n✅\nso-so\n❌\n❌\n\n\nIncremental reads/writes\nYes, but not supported by current Python libs\n✅\n✅\nYes (but not random access)\n\n\nSupports very large and high-dimensional datasets\n✅\n✅\n❌ (limited nr. columns per table)\n✅ Storing tensors requires flattening.\n\n\nSimplicity\n✅\n❌ (basically full file system)\n✅ (it’s a database)\n✅\n\n\nMetadata support\nOnly minimal\n✅\n✅\n❌ (requires separate  metadata file)\n\n\nMaintenance\nApache project, open and quite active\nClosed group, but active community on Jira and conferences\nRun by a company. Uses an email list.\n✅\n\n\nAvailable examples of usage in ML\n✅\n✅\n❌\n✅\n\n\nFlexibility\nOnly tabular\nVery flexible, maybe too flexible\nRelational multi-table\nOnly tabular\n\n\nVersioning/Diff\nOnly via S3 or delta lake\n❌\n❌\n✅\n\n\nDifferent length vectors\nAs blob\n✅\n❌ ?\n✅\n\n\n\nPerformance benchmarks\nThere exist some prior benchmarks (here and here) on storing dataframes. These only consider single-table datasets. For reading/writing, CSV is clearly slower and Parquet is clearly faster. For storage, Parquet is most efficient but zipped CSV as well. HDF requires a lot more disk space. We also ran our own benchmark to compare the writing performance of those data formats for very large and complex machine learning datasets, but could not find a way to store these in one file in Parquet.\nVersion control\nVersion control for large datasets is tricky. For text-based formats (CSV), we could use git LFS store the datasets and have automated versioning of datasets. We found it quite easy to export all current OpenML dataset to GitLab: https://gitlab.com/data/d/openml.\nThe binary formats do not allow us to track changes in the data, only to recover the exact versions of the datasets you want (and their metadata). Potentially, extra tools could still be used to export the data to dataframes or text and then compare them. Delta Lake has version history support, but seemingly only for Spark operations done on the datasets.\nWe need your help! If we have missed any format we should investigate, or misunderstood those we have investigated, or missed some best practice, please tell us. You are welcome to comment below, or send us an email at openmlhq@googlegroups.com\nContributors to this blog post: Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren, Pieter Gijsbers, Andreas Mueller, Matthias Feurer, Jan van Rijn, Marcus Weimer, Marcel Wever, Gertjan van den Burg, Nick Poorman"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Open Machine Learning blog",
    "section": "",
    "text": "Welcome to the Open Machine Learning blog\nThis blog brings you stories about OpenML: why we want to streamline machine learning research, how you can use it, and what we are doing. We are also open to your stories about anything related to open machine learning research, education, and applications. Join the conversation :).\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nExperiments with Temperature\n\n\n\n\n\n\nllm\n\n\n\nExperimenting with LLM temperature and its effects on answer quality\n\n\n\n\n\nJul 8, 2024\n\n\nSubhaditya Mukherjee\n\n\n\n\n\n\n\n\n\n\n\n\nParticipate in the OpenML Hacktoberfest Workshop!\n\n\n\n\n\n\nOpenML\n\n\nHacktoberfest\n\n\nWorkshop\n\n\n\nJoin us at the 2020 Fall Workshop on gather.town!\n\n\n\n\n\nOct 23, 2020\n\n\nPieter Gijsbers, Prabhant Singh\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible deep learning with OpenML\n\n\n\n\n\n\nopenml\n\n\ndeep learning\n\n\n\nReleasing OpenML Deep learning libraries compatible with keras, pytorch and mxnet.\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFinding a standard dataset format for machine learning\n\n\n\n\n\n\nOpenML\n\n\nData\n\n\n\nExploring new dataset format options for OpenML.org\n\n\n\n\n\nMar 23, 2020\n\n\nPieter Gijsbers, Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren\n\n\n\n\n\n\n\n\n\n\n\n\nOpenML - Machine Learning as a community\n\n\n\n\n\n\nOpenML\n\n\n\nA description of how OpenML fits into traditional ML practices\n\n\n\n\n\nOct 26, 2019\n\n\nNeeratyoy Mallik\n\n\n\n\n\n\n\n\n\n\n\n\nOpenML workshop at Dagstuhl\n\n\n\n\n\n\nOpenML\n\n\n\nOpenML workshop at Dagstuhl\n\n\n\n\n\nOct 24, 2019\n\n\nHeidi Seibold\n\n\n\n\n\n\n\n\n\n\n\n\nBasic components of OpenML\n\n\n\n\n\n\nOpenML\n\n\n\nUnderstanding the building blocks of OpenML\n\n\n\n\n\nMar 3, 2017\n\n\nJan van Rijn\n\n\n\n\n\n\n\n\n\n\n\n\nmlr loves OpenML\n\n\n\n\n\n\nOpenML\n\n\nmlr\n\n\nR\n\n\n\nOpenML with mlr\n\n\n\n\n\nSep 18, 2016\n\n\nHeidi Seibold\n\n\n\n\n\n\n\n\n\n\n\n\nOpenML\n\n\n\n\n\n\nOpenML\n\n\n\nIntroduction to OpenML\n\n\n\n\n\nMay 2, 2016\n\n\nHeidi Seibold\n\n\n\n\n\n\nNo matching items"
  }
]